{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushikeshnaik779/new_water/blob/main/mistral_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGH_TRzgGDd3"
      },
      "source": [
        "# âš¡ Deploy Mistral (Mixtral-8x7b) in a Notebook\n",
        "Below is working code for running mixtral-8x7b locally (in a python notebook) with minimal requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’» Install requirements\n",
        "Use huggingface-hub and llama-cpp-python to download the model and interface with it using llama-cpp (for inference)"
      ],
      "metadata": {
        "id": "SxFry8zt8fsm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgiKTHOL6WXR"
      },
      "outputs": [],
      "source": [
        "!pip3 install huggingface-hub\n",
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_GoDf2_6ZCA"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download TheBloke/dolphin-2.5-mixtral-8x7b-GGUF dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’¬ Chat with LLM\n",
        "Set up a simple chat interaction using the mixtral artifact wrapped with llama-cpp python bindings. This may take a few minutes to load as the model since the artifact is downloaded.\n",
        "\n",
        "It is recommended to use GPU accelerated runtime (i.e. V100)\n",
        "\n",
        "Credit for below inference code goes to: https://huggingface.co/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/blob/main/README.md\n"
      ],
      "metadata": {
        "id": "WeQ-7zgz9AxN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XEq7iEFKEIIM"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "import os\n",
        "\n",
        "# Set the path to your downloaded model file\n",
        "model_path = \"dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\"\n",
        "\n",
        "# Initialize the Llama model\n",
        "# Adjust these parameters based on your system capabilities and the model's requirements\n",
        "# Below should work for V100 runtime\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=32768,  # Max sequence length\n",
        "    n_threads=2,  # Number of CPU threads\n",
        "    n_gpu_layers=10  # Number of layers to offload to GPU (set to 0 if no GPU)\n",
        ")\n",
        "\n",
        "prompt = \"system\\n{system_message}\\nuser\\n{prompt}\\nassistant\"\n",
        "\n",
        "# Function to append messages to the prompt and get the model's response\n",
        "def chat_with_model(llm, prompt, user_message):\n",
        "    # Append the user's message to the prompt\n",
        "    updated_prompt = prompt + \"\\nuser\\n\" + user_message + \"\\nassistant\"\n",
        "\n",
        "    # Generate the model's response\n",
        "    output = llm(\n",
        "        updated_prompt,\n",
        "        max_tokens=512,  # Adjust as needed\n",
        "        stop=[\"</s>\"],   # Stop token, adjust as needed\n",
        "        echo=True        # Echo the prompt in the output\n",
        "    )\n",
        "\n",
        "    # Extract only the model's response from the dictionary\n",
        "    full_response = output['choices'][0]['text'].strip()\n",
        "\n",
        "    # Extract the latest response (after the last \"assistant\" tag)\n",
        "    latest_response = full_response.split(\"assistant\\n\")[-1].strip()\n",
        "\n",
        "    # Append the user's message and the model's latest response to the prompt for the next iteration\n",
        "    updated_prompt += \"\\nuser\\n\" + user_message + \"\\nassistant\\n\" + latest_response\n",
        "\n",
        "    return updated_prompt, latest_response\n",
        "\n",
        "# Start the chat interaction\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_message = input(\"You: \")\n",
        "\n",
        "    # Check for a special command to end the chat (e.g., 'quit')\n",
        "    if user_message.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Get model response and update the prompt\n",
        "    prompt, model_response = chat_with_model(llm, prompt, user_message)\n",
        "    print(\"Model:\", model_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmiFTE4XFIns"
      },
      "outputs": [],
      "source": [
        "# double check the correct llm is loaded in\n",
        "print(llm)\n",
        "print(\"Model being used:\", model_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}